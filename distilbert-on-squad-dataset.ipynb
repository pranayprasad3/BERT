{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Question Answering with SQuAD 2.0 on DistilBERT"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install transformers\nfrom torchvision.datasets.utils import download_url","execution_count":1,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.5.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\nRequirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers) (3.14.0)\nRequirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.11.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dowload the dataset\ndataset_url_train = \" https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\" \ndataset_url_test = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\"\ndownload_url(dataset_url_train, '.')\ndownload_url(dataset_url_test, '.')","execution_count":2,"outputs":[{"output_type":"stream","text":"Downloading  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json to ./train-v2.0.json\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00fe325777a4454f89babc25e0042308"}},"metadata":{}},{"output_type":"stream","text":"Downloading https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json to ./dev-v2.0.json\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51173f382c22477b821867352353ae30"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Each split is in a structured json file with a number of questions and answers for each passage (or context). We’ll take this apart into parallel lists of contexts, questions, and answers :\nNote that the contexts here are repeated since there are multiple questions per context."},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nfrom pathlib import Path\n\ndef read_squad(path):\n    path = Path(path)\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    contexts = []\n    questions = []\n    answers = []\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    contexts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n\n    return contexts, questions, answers\n\ntrain_contexts, train_questions, train_answers = read_squad('train-v2.0.json')\nval_contexts, val_questions, val_answers = read_squad('dev-v2.0.json')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(train_contexts))\nprint(len(train_questions))\nprint(len(train_answers))","execution_count":4,"outputs":[{"output_type":"stream","text":"86821\n86821\n86821\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(val_contexts))\nprint(len(val_questions))\nprint(len(val_answers))","execution_count":5,"outputs":[{"output_type":"stream","text":"20302\n20302\n20302\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'{train_contexts[0]} \\n')\nprint(f'{train_questions[0]} \\n')\nprint(train_answers[0])","execution_count":19,"outputs":[{"output_type":"stream","text":"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". \n\nWhen did Beyonce start becoming popular? \n\n{'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The contexts and questions are just strings. The answers are dicts containing the subsequence of the passage with the correct answer as well as an integer indicating the character at which the answer begins. In order to train a model on this data we need (1) the tokenized context/question pairs, and (2) integers indicating at which token positions the answer begins and ends"},{"metadata":{},"cell_type":"markdown","source":"## First, let’s get the character position at which the answer ends in the passage (we are given the starting position). Sometimes SQuAD answers are off by one or two characters, so we will also adjust for that."},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_end_idx(answers, contexts):\n    for answer, context in zip(answers, contexts):\n        gold_text = answer['text']\n        start_idx = answer['answer_start']\n        end_idx = start_idx + len(gold_text)\n\n        # sometimes squad answers are off by a character or two – fix this\n        if context[start_idx:end_idx] == gold_text:\n            answer['answer_end'] = end_idx\n        elif context[start_idx-1:end_idx-1] == gold_text:\n            answer['answer_start'] = start_idx - 1\n            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n        elif context[start_idx-2:end_idx-2] == gold_text:\n            answer['answer_start'] = start_idx - 2\n            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n\nadd_end_idx(train_answers, train_contexts)\nadd_end_idx(val_answers, val_contexts)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check our train_answers once again to see the changes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'{train_contexts[0]} \\n')\nprint(f'{train_questions[0]} \\n')\nprint(train_answers[0])","execution_count":20,"outputs":[{"output_type":"stream","text":"Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\". \n\nWhen did Beyonce start becoming popular? \n\n{'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(train_contexts[1])\nprint(train_questions[1])\nprint(train_answers[1])","execution_count":9,"outputs":[{"output_type":"stream","text":"What areas did Beyonce compete in when she was growing up?\n{'text': 'singing and dancing', 'answer_start': 207, 'answer_end': 226}\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Let’s tokenize our context/question pairs\n\nTokenizers can accept parallel lists of sequences and encode them together as sequence pairs."},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\ntrain_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\nval_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41a0af02978549cb9894ed37e36edb90"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167079dae62543a4805a2f81fae8317f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We can't view these encoded sequence pairs\ntrain_encodings[0:5]","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Converting our character start/end positions to token start/end positions\n\nWhen using Hugging Face Fast Tokenizers, we can use the built in char_to_token() method"},{"metadata":{"trusted":true},"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n        # if None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n        if end_positions[-1] is None:\n            end_positions[-1] = tokenizer.model_max_length\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our data is ready."},{"metadata":{},"cell_type":"markdown","source":"## Let’s just put our encodings and answers in a PyTorch dataset.\n\nIn PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing len and getitem. We put the data in this format so that the data can be easily batched such that each key in the batch encoding corresponds to a named parameter of the forward() method of the model we will train."},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n\ntrain_dataset = SquadDataset(train_encodings)\nval_dataset = SquadDataset(val_encodings)","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Now we can use a DistilBert model with a QA task-specific final layers for training:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import DistilBertForQuestionAnswering\nmodel = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe952883c66545268d4a9a9edf269eb6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b689d647634040aeb4b6b62bcc585076"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"The data and model are both ready to go."},{"metadata":{},"cell_type":"markdown","source":"## Fine-tuning with Trainer\nThe training took about 4.5 hours to complete"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.notebook import tqdm","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel.to(device)\nmodel.train()\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\noptim = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(3):\n    for batch in tqdm(train_loader):\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device) \n        attention_mask = batch['attention_mask'].to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n\nmodel.eval()","execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5427.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c61a0c79dba9426c8b0287f5ac9a9019"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5427.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25654914022a4fcba9b2cdb45fc26dde"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5427.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38dd7ceb295044a1b6a7756d020d7aa6"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"DistilBertForQuestionAnswering(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Saving the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\n# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n\noutput_dir = './model_save/'\n\n# Create output directory if needed\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nprint(\"Saving model to %s\" % output_dir)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Good practice: save your training arguments together with the trained model\n# torch.save(args, os.path.join(output_dir, 'training_args.bin'))","execution_count":18,"outputs":[{"output_type":"stream","text":"Saving model to ./model_save/\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"('./model_save/tokenizer_config.json',\n './model_save/special_tokens_map.json',\n './model_save/vocab.txt',\n './model_save/added_tokens.json')"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}